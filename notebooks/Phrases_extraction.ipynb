{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Основная идея:\n",
    "    1) Берём BENEFITS и DRAWBACKS в которых хорошо описаны основные характеристики товаров\n",
    "    2) Разбиваем эти описания на отдельные фразы\n",
    "    3) Используя коллокации и алгоритм fp-grow извлекаем часто встречающиеся наборы фраз (далее будем говорить ключевые слова)\n",
    "    5) Строим MinhashLSH индекс этих ключевых фраз\n",
    "    4) Функция predict:\n",
    "        4.1) На вход принимаем все комментарии определённого товара, разбиваем на фразы\n",
    "        4.2) Используя MinhashLSH мы матчим текущую фразу с ключевыми словами с помощью LSH индекса (по Джакарду)\n",
    "        4.3) Если есть хоть одна детекция ключевой фразы, добавляем ТЕКУЩУЮ ФРАЗУ в список детекций\n",
    "        4.4) Дальше идёт стадия переранжирования\n",
    "    5) Переранжирование (используется результат Category_model_importance.ipynb)\n",
    "        5.1) Для каждого товара есть категория\n",
    "        5.2) Строится модель многокласовой классификации которая по тексту отзыва предсказывает категорию товара\n",
    "        5.3) В коэффициентах модели слова которые имеют высокий коэффициент наиболее релевантны для описания это категории\n",
    "        5.4) Используя эти коэффициенты мы переранжируем выдачу predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "# custom imports\n",
    "SCRIPTS_PATH = '../scripts'\n",
    "if SCRIPTS_PATH not in sys.path:\n",
    "    sys.path.append(SCRIPTS_PATH)\n",
    "    \n",
    "import data_preparation\n",
    "from text_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset1_df = pd.read_csv('../data/new_data/dataset1.csv', index_col=0) # отзывы\n",
    "dataset2_df = pd.read_csv('../data/new_data/dataset2.csv', index_col=0) # характеристики товаров\n",
    "\n",
    "compare_v3_df = pd.read_csv('../data/new_data/compare_v3.csv', sep=';', index_col=0) # сравнения товаров \n",
    "views_df = pd.read_csv('../data/new_data/views.csv', sep=';', index_col=0) # просмотры (сессия) пользователей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text = list()\n",
    "\n",
    "raw_text.extend(dataset1_df['BENEFITS'].dropna())\n",
    "raw_text.extend(dataset1_df['DRAWBACKS'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103500/103500 [00:27<00:00, 3703.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200551"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentences = list()\n",
    "\n",
    "for text in tqdm(raw_text):\n",
    "    raw_sentences.extend(map(lambda x: tokenize(x, drop_numbers=True), get_sentences(text)))\n",
    "    \n",
    "raw_sentences = [x for x in raw_sentences if len(x)>0]\n",
    "len(raw_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.summarization.textcleaner import clean_text_by_sentences, clean_text_by_word\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "\n",
    "bigrams = Phrases(raw_sentences, threshold=10.)\n",
    "bigram_phraser = Phraser(bigrams)\n",
    "trigram = Phrases(bigram_phraser[raw_sentences], threshold=10.)\n",
    "trigram_phraser = Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3349"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = list()\n",
    "for words in bigram_phraser[raw_sentences]:\n",
    "    for word in words:\n",
    "        if '_' in word:\n",
    "            keywords.append(word)\n",
    "            \n",
    "keywords = [x.split('_') for x in set(keywords)]\n",
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['меню', 'настроек']\n",
      "['навигация', 'меню']\n",
      "['простое', 'меню']\n",
      "['меню', 'простое']\n",
      "['неудобное', 'меню']\n",
      "['удобное', 'меню']\n",
      "['понятное', 'меню']\n",
      "['меню', 'русском']\n",
      "['доступное', 'меню']\n"
     ]
    }
   ],
   "source": [
    "for kw in keywords:\n",
    "    if 'меню' in kw:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP-growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2989\n"
     ]
    }
   ],
   "source": [
    "import fim\n",
    "patterns = fim.fpgrowth(raw_sentences, report = 'S', supp=0.008) # 0.008 - only BENEFITS\n",
    "print(len([x for x in patterns if len(x[0])>1]))\n",
    "\n",
    "for item in all_phrases:\n",
    "    if 'обучения' in item:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('внешний', 'вид'), 0.7055561926891415),\n",
       " (('``', \"''\"), 0.5270479828073658),\n",
       " (('цена', 'качество'), 0.3295919741113233),\n",
       " (('сборки', 'качество'), 0.31014554901247066),\n",
       " (('красивый', 'дизайн'), 0.3066551650203689),\n",
       " (('хороший', 'звук'), 0.29718126561323555),\n",
       " (('удобно', 'очень'), 0.23086396976330212),\n",
       " (('хорошее', 'качество'), 0.21241479723362136),\n",
       " (('держит', 'долго'), 0.2064312818185898),\n",
       " (('большой', 'экран'), 0.20144501897273012)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([x for x in patterns if len(x[0])>1], key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6338/6338 [00:10<00:00, 617.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=256) # threshold=0.5, num_perm=256\n",
    "\n",
    "def get_hash(keywords):\n",
    "    keywords = set(keywords)\n",
    "    mhash = MinHash(num_perm=256)\n",
    "    for item in keywords:\n",
    "        mhash.update(item.encode('utf8'))\n",
    "    return mhash\n",
    "\n",
    "all_phrases = list()\n",
    "# all_phrases.extend(raw_sentences)\n",
    "all_phrases.extend([x[0] for x in patterns if len(x[0])>1])\n",
    "all_phrases.extend(keywords)\n",
    "\n",
    "for i, item in tqdm(enumerate(all_phrases), total=len(all_phrases)):\n",
    "    lsh.insert(str(i), get_hash(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['вся', 'игра']\n",
      "('вся', 'игра')\n",
      "('крутая', 'игра')\n",
      "('затягивает', 'игра')\n",
      "('игра', 'хорошая')\n",
      "('игра', 'графика')\n",
      "('прикольная', 'игра')\n"
     ]
    }
   ],
   "source": [
    "for idx in lsh.query(get_hash(set(tokenize('игра')))):\n",
    "    print(all_phrases[int(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "очень прост в обслуживании\n",
      "все понятно интуитивно\n",
      "вся пыль осталась в воде\n",
      "все на своих местах\n",
      "тест-драйв длился 30 минут\n",
      "но пока покупкой доволен\n",
      "а весь мусор накапливался просто в корпусе пылесоса\n",
      "Ни во время уборки ни после\n",
      "либо эйфория после покупки\n",
      "фильтр пылесоса\n",
      "держась одной рукой ручку\n",
      "Купил несколько дней назад\n",
      "Отличный пылесос\n",
      "Пылесос отличный\n",
      "длинный шнур\n",
      "легко разбирается-собирается\n",
      "легко моется\n",
      "на мой взгляд хорошая\n",
      "очень удобное управление на ручке\n",
      "Что бы долго работал\n",
      "Всем рекомендую\n",
      "Купил полгода назад\n",
      "Сейчас практически не тянет\n",
      "Отличный пылесос\n",
      "шерсть собирает влёт)))\n",
      "регулировка мощности\n",
      "Берите не пожалеете!\n",
      "пылесос просто замечательный! Он очень юркий\n",
      "Управление на ручке очень удобное\n",
      "С уборкой теперь нет никаких проблем!\n",
      "Покупка порадовала очень\n",
      "Надеюсь радовать будет долго\n",
      "Пылесос супер!!! Убирает отлично\n",
      "очень легко моется\n",
      "прост в обращении\n",
      "Единственный недостаток немного тяжеловат\n",
      "пластмасс дешевый\n",
      "Пылесос отличный\n",
      "Мощности хватает\n",
      "Я не сожалею: пылесос хороший\n",
      "Результат на лицо\n",
      "Если убираетесь дома не каждый день\n",
      "приходится постоянно его мыть\n",
      "Мощность вообще маленькая\n",
      "что лучше мешкового пылесоса нет ничего\n",
      "Что сказать отличный аппарат\n",
      "качественная сборка внешний вид суперский\n",
      "А так покупкой доволен\n",
      "Привезла доставка\n",
      "Качество пластика\n",
      "В общем пока очень доволен\n",
      "немного подождать\n",
      "Сегодня купил\n",
      "тоже время тратится\n",
      "Мыть удобно\n",
      "Пока не нравитс\n",
      "Впечатления только положительные\n",
      "До этого был Samsung с мешком для сбора пыли\n",
      "правдой более 10 лет\n",
      "им очень довольны\n",
      "большая проблема! Мощности хватает с лихвой\n",
      "Отличный пылесос:\n",
      "Немного тяжеловат\n",
      "более 10 кг\n",
      "Отличный пылесос\n",
      "Со своей работой справляется отлично\n"
     ]
    }
   ],
   "source": [
    "TEST_PRODUCT_ID = 20022045\n",
    "\n",
    "test_product = dataset1_df[dataset1_df['PRODUCT']==TEST_PRODUCT_ID]\n",
    "test_product_all_text = '.'.join(list(test_product['TEXT']))\n",
    "test_product_category_name = test_product['CATEGORY_NAME'].iloc[0]\n",
    "\n",
    "extracted_keywords = list()\n",
    "for sent in get_sentences(test_product_all_text):\n",
    "    tokenized_sent = set(tokenize(sent))\n",
    "    matches = lsh.query(get_hash(tokenized_sent))\n",
    "    if len(matches)>0 and len(tokenized_sent)>1:\n",
    "        extracted_keywords.append(sent.strip())\n",
    "        print(sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#category_keywords = pickle.load(open('../dumps/category_keywords.pkl', 'rb'))\n",
    "#lsh = pickle.load(open('../dumps/minhash_lsh.pkl', 'rb'))\n",
    "#stop_words = pickle.load(open('../dumps/stop_words.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Экран у него не такой четкий',\n",
       " 'Батарея держит отлично',\n",
       " 'Очень шустрый аппарат',\n",
       " 'Цена завышена даже очень!!',\n",
       " 'Всё отлично',\n",
       " 'вот только батарейка быстро садится',\n",
       " ') 2) Фронтальная камера 2 МПикс',\n",
       " ') 3) Большой яркий экран 4',\n",
       " '(Лично мне нравится насыщенные цвета',\n",
       " '(Ну это как обычно у Samsung',\n",
       " '(Нормального размера! А то мучайся: бегай',\n",
       " '(Удобно лежит в руке',\n",
       " 'лично мне удобно пользоваться одной рукой',\n",
       " 'Задняя крышка настолько хрупкая',\n",
       " ') 3) Объём встроенной памяти',\n",
       " 'Выход: прошивка',\n",
       " ') Комментарий: Лично я покупкой довольна',\n",
       " 'но девайс меня всё равно впечатлил',\n",
       " 'Фотографии хорошие',\n",
       " 'Из недостатков: батарея',\n",
       " 'Сразу после покупки',\n",
       " 'когда я просто звонила',\n",
       " 'Купил жене год назад',\n",
       " 'Самый удобный андройд',\n",
       " 'что телефон просто лежит в режиме ожидания!\\nПри том',\n",
       " '1) Цвета не естественные',\n",
       " 'Особенно зеленый',\n",
       " 'Даже на максимальной яркости',\n",
       " 'Не буду описывать плюсы',\n",
       " 'Экран просто великолепный',\n",
       " 'цвета яркие',\n",
       " 'сенсор тоже очень порадовал',\n",
       " 'видео снимает просто отличного качества',\n",
       " 'различные сигналы )))) аккумулятор держит заряд хорошо',\n",
       " 'конечно это зависит от того как использовать телефон',\n",
       " 'приложения работают быстро',\n",
       " 'их огромный выбор',\n",
       " 'цена на него стоит приемлемая',\n",
       " 'он вполне стоит этих денег',\n",
       " 'он не удобен в использовании в том плане',\n",
       " 'что все нужно постоянно в айтюнс переводить',\n",
       " 'экран просто сказка!',\n",
       " 'Телефон очень хороший',\n",
       " 'Для меня телефон удобный',\n",
       " 'Цена приятная',\n",
       " 'Большое количество полезных приложений',\n",
       " 'Экран - он действительно классный',\n",
       " 'Железо - качественное',\n",
       " 'Комментарии: Модель действительно очень достойная',\n",
       " 'Данный телефон просто супер',\n",
       " 'минусов пока что никаких не заметила',\n",
       " 'пользуюсь уже 2 месяца',\n",
       " 'все просто замечательно; после покупки себе',\n",
       " 'Аппарат просто сказка',\n",
       " 'Очень радует использование интернета',\n",
       " 'Единственный минус аккумулятор',\n",
       " 'да легкий слишком',\n",
       " 'Друг работает в связном вот',\n",
       " 'вот теперь Samsung Galaxy S2',\n",
       " 'Экран яркий',\n",
       " 'Не самый приятный экран',\n",
       " 'металл куда приятнее держать в руке',\n",
       " 'Долго ждал',\n",
       " '- USB OTG',\n",
       " 'Телефон просто Лучший',\n",
       " 'выскальзывает из рук',\n",
       " 'в руке сидит не очень',\n",
       " 'Привет всем! Телефон просто сказка',\n",
       " 'Экран супер',\n",
       " 'приятно тонкий дизайн',\n",
       " 'приобрел пару месяцев назад',\n",
       " 'все 6 вполне устраивает',\n",
       " 'экран отличный',\n",
       " 'управление простое',\n",
       " 'Работает супер',\n",
       " 'Купил три дня назад',\n",
       " 'мобильный интернет - красота',\n",
       " 'в руке держать удобней',\n",
       " 'то телефон отличный',\n",
       " 'кроме цены',\n",
       " 'но цвета очень не живые\\n-+Смартфон очень легкий',\n",
       " 'но не очень удобно лежит в руке',\n",
       " 'менял настройки',\n",
       " 'на самом деле',\n",
       " 'Очень слабый звук (не айс для меломанов)',\n",
       " 'все становится понятно',\n",
       " 'Экран большой',\n",
       " 'Небольшой минус',\n",
       " 'экран у него такой маленький',\n",
       " 'те кто говорит',\n",
       " 'всё в таком роде',\n",
       " 'могу сказать',\n",
       " 'телефон не зависал',\n",
       " 'С уверенностью могу сказать',\n",
       " 'я бы сказал лучший по всем параметрам',\n",
       " 'Хорошая сборка',\n",
       " 'Советую брать этот аппарат',\n",
       " 'Интернет летает',\n",
       " 'отклик телефона на команды быстрый',\n",
       " 'экран яркий',\n",
       " 'Довольны всем',\n",
       " 'что быстро садится батарея',\n",
       " 'Телефон супер',\n",
       " 'аппарат супер',\n",
       " 'во многом очень хорош']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string, pickle\n",
    "from datasketch import MinHash\n",
    "import nltk\n",
    "\n",
    "#category_keywords = pickle.load(open('../dumps/category_keywords.pkl', 'rb'))\n",
    "#lsh = pickle.load(open('../dumps/minhash_lsh.pkl', 'rb'))\n",
    "#stop_words = pickle.load(open('../dumps/stop_words.pkl', 'rb'))\n",
    "\n",
    "class PhrasesExtractor:\n",
    "    def __init__(self, minhash, category_keywords, stop_words):\n",
    "        self.category_keywords = category_keywords\n",
    "        self.minhash = minhash\n",
    "        self.stop_words = stop_words\n",
    "        self.punctuation = string.punctuation + '»«-–—`\\'()'\n",
    "    \n",
    "    def tokenize(self, file_text, use_lower=True, drop_stopwords=True, use_stemmer=False, drop_numbers=False):\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "        tokens = [x for x in tokens if ( x not in self.punctuation )]\n",
    "\n",
    "        if use_lower:\n",
    "            tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        if drop_numbers:\n",
    "            tokens = [x for x in tokens if ( x not in string.digits )]\n",
    "\n",
    "        if drop_stopwords:\n",
    "            tokens = [x for x in tokens if ( x not in self.stop_words )]\n",
    "\n",
    "        if use_stemmer:\n",
    "            pass\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def get_sentences(self, text):\n",
    "        if '.' in text:\n",
    "            for sent in text.split('.'):\n",
    "                if ',' in sent:\n",
    "                    for sent_2 in sent.split(','):\n",
    "                        if ' и ':\n",
    "                            for sent_3 in sent_2.split(' и '):\n",
    "                                yield sent_3.strip()\n",
    "                        else:\n",
    "                            yield sent_2.strip()\n",
    "                else:\n",
    "                    yield sent.strip()\n",
    "        else:\n",
    "            yield text\n",
    "    \n",
    "    def get_hash(self, keywords):\n",
    "        keywords = set(keywords)\n",
    "        mhash = MinHash(num_perm=256)\n",
    "        for item in keywords:\n",
    "            mhash.update(item.encode('utf8'))\n",
    "        return mhash\n",
    "    \n",
    "    def get_category_keyword_score(self, category, keyword):\n",
    "        category = category.strip()\n",
    "        assert(category in self.category_keywords)\n",
    "        current_category_keywords = self.category_keywords[category]\n",
    "        if keyword in current_category_keywords:\n",
    "            index = list(current_category_keywords.keys()).index(keyword)\n",
    "            score = current_category_keywords[keyword]\n",
    "            return index, score\n",
    "        else:\n",
    "            return -1, 0\n",
    "    \n",
    "    def filter_phrases_old(self, extracted_keywords, test_product_category_name):\n",
    "        assighned_phrases = dict()\n",
    "        for sent in extracted_keywords:\n",
    "            for word in self.tokenize(sent):\n",
    "                idx, current_score = self.get_category_keyword_score(test_product_category_name, word)\n",
    "                if idx == -1:\n",
    "                    continue\n",
    "                if idx in assighned_phrases:\n",
    "                    if current_score>assighned_phrases[idx][1]:\n",
    "                        assighned_phrases[idx] = (sent, current_score)\n",
    "                else:\n",
    "                    assighned_phrases[idx] = (sent, current_score)\n",
    "        return assighned_phrases\n",
    "    \n",
    "    def filter_phrases(self, extracted_keywords, test_product_category_name):\n",
    "        assighned_phrases = dict()\n",
    "        for sent in extracted_keywords:\n",
    "            if 'купил' in sent:\n",
    "                continue\n",
    "            keywords = [self.get_category_keyword_score(test_product_category_name, word) for word in self.tokenize(sent)]\n",
    "            idx, current_score = sorted(keywords, key=lambda x: x[1], reverse=True)[0]\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            if idx in assighned_phrases:\n",
    "                if current_score>assighned_phrases[idx][1]:\n",
    "                    assighned_phrases[idx] = (sent, current_score)\n",
    "            else:\n",
    "                assighned_phrases[idx] = (sent, current_score)\n",
    "        return assighned_phrases\n",
    "            \n",
    "        \n",
    "    def predict(self, df, top_n=5):\n",
    "        test_product_all_text = '.'.join(list(df['TEXT']))\n",
    "        test_product_category_name = df['CATEGORY_NAME'].iloc[0]\n",
    "\n",
    "        extracted_keywords = list()\n",
    "        for sent in self.get_sentences(test_product_all_text):\n",
    "            tokenized_sent = set(self.tokenize(sent))\n",
    "            matches = self.minhash.query(self.get_hash(tokenized_sent))\n",
    "            if len(matches)>0 and len(tokenized_sent)>1:\n",
    "                extracted_keywords.append(sent.strip())\n",
    "        if top_n != None and len(extracted_keywords)>top_n:\n",
    "            filtered_phrases = self.filter_phrases(extracted_keywords, test_product_category_name)\n",
    "            filtered_phrases = [filtered_phrases[key][0] for key in sorted(filtered_phrases.keys())]\n",
    "            return filtered_phrases[:top_n]\n",
    "        else:\n",
    "            return extracted_keywords\n",
    "\n",
    "phrases_extractor = PhrasesExtractor(minhash=lsh, category_keywords=category_keywords, stop_words=stop_words)\n",
    "\n",
    "test_product = dataset1_df[dataset1_df['PRODUCT']==30013578]\n",
    "phrases_extractor.predict(test_product, top_n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cells below are useless and werent included in solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters_keywords = list()\n",
    "\n",
    "for sent in extracted_keywords:\n",
    "    for word in sent:\n",
    "        idx, score = get_category_keyword_score(test_product_category_name, word)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gensim\n",
    "\n",
    "# https://nlpub.ru/Russian_Distributional_Thesaurus#gensim_.3E.3D_1.0.1\n",
    "\n",
    "# 100 all.norm-sz100-w10-cb0-it1-min100.w2v\n",
    "# 500 all.norm-sz500-w10-cb0-it3-min5.w2v\n",
    "\n",
    "w2v_fpath = \"../../../NLP/word4vec/w2v_ru/all.norm-sz100-w10-cb0-it1-min100.w2v\"\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(w2v_fpath, binary=True, unicode_errors='ignore')\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "w2v_size = w2v['дом'].size\n",
    "\n",
    "w2v.similar_by_word('принтер')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_vectors = list()\n",
    "\n",
    "for sent in get_sentences(test_product_all_text):\n",
    "    sent_preprocessed = set(tokenize(sent))\n",
    "    matches = lsh.query(get_hash(sent_preprocessed))\n",
    "    if len(matches)>0:\n",
    "        word_vectors = [w2v[word] for word in sent_preprocessed if word in w2v]\n",
    "        if len(word_vectors) == 0:\n",
    "            word_vectors = [np.zeros(w2v_size)]\n",
    "        sent_vectors.append((sent, np.mean(word_vectors, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=8, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "N_CLUSTERS = min(len(sent_vectors), 8)\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS)\n",
    "kmeans.fit([x[1] for x in sent_vectors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select best item from cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "test_product_category_name = test_product['CATEGORY_NAME'].iloc[0]\n",
    "sorted_clusters = sorted(zip(kmeans.labels_, sent_vectors), key=lambda x:x[0])\n",
    "\n",
    "test_product_aggregated_result = dict()\n",
    "for cluster, sentences in groupby(sorted_clusters, key=lambda x: x[0]):\n",
    "    cluster_phrases = list()\n",
    "    for cluster, (sent, w2v_vector) in sentences:\n",
    "        sent_total_score = 0\n",
    "        for word in tokenize(sent):\n",
    "            word_cat_score = get_category_keyword_score(test_product_category_name, word)[1]\n",
    "            sent_total_score += word_cat_score\n",
    "        cluster_phrases.append((sent, sent_total_score))\n",
    "    test_product_aggregated_result[cluster] = cluster_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  Ни во время уборки ни после 0.359015470675\n",
      "  Если убираетесь дома не каждый день 0.257453089684\n",
      "  С уборкой теперь нет никаких проблем! 0.148038591659\n",
      "  тест-драйв длился 30 минут 0.100685811633\n",
      "  Через день 0.0462654816153\n",
      "  Проработал год 0.0376872428525\n",
      "  Пока не нравитс 0.0176919469655\n",
      "  Что бы долго работал -0.00495883627949\n",
      "  Сегодня купил -0.0126162482107\n",
      "  Купил несколько дней назад -0.0347118618772\n",
      "  Надеюсь радовать будет долго -0.0396582528018\n",
      "  немного подождать -0.0499852659973\n",
      "  Купил полгода назад -0.0603150919654\n",
      "1\n",
      "  Пылесос супер!!! Убирает отлично 0.264338987061\n",
      "  шерсть собирает влёт))) 0.239768269764\n",
      "  а весь мусор накапливался просто в корпусе пылесоса 0.234064298466\n",
      "  Я не сожалею: пылесос хороший 0.112066078143\n",
      "  Купил пылесос сегодня 0.100862658052\n",
      "  что лучше мешкового пылесоса нет ничего 0.0841263157607\n",
      "  у меня дома кот 0.0832005024348\n",
      "  Купила пылесос недавно 0.0692756632316\n",
      "  вся пыль осталась в воде 0.0663154645262\n",
      "  Отличный пылесос 0.0640469611648\n",
      "  Пылесос отличный 0.0640469611648\n",
      "  Отличный пылесос 0.0640469611648\n",
      "  Пылесос отличный 0.0640469611648\n",
      "  Отличный пылесос: 0.0640469611648\n",
      "  Отличный пылесос 0.0640469611648\n",
      "  фильтр пылесоса 0.054125555981\n",
      "  Шумный 0.0421010350856\n",
      "  пылесос просто замечательный! Он очень юркий 0.0320990627044\n",
      "  земля 0.0255828223012\n",
      "  держась одной рукой ручку 0.0216778606466\n",
      "  пластмасс дешевый 0.0102735968728\n",
      "  легко моется -0.0191568585112\n",
      "  Что сказать отличный аппарат -0.0244218487244\n",
      "  корпус -0.0246171023252\n",
      "  но открыв крышку -0.0253980255445\n",
      "  шланг -0.0334350172981\n",
      "  До этого был Samsung с мешком для сбора пыли -0.0601980565443\n",
      "  Мыть удобно -0.0643054318766\n",
      "  что он тихий -0.0729633874011\n",
      "  длинный шнур -0.118959234559\n",
      "  фильтр часто забивается -0.122066838109\n",
      "  приходится постоянно его мыть -0.160322611168\n",
      "  очень удобное управление на ручке -0.181551662676\n",
      "  Управление на ручке очень удобное -0.181551662676\n",
      "2\n",
      "  но пока покупкой доволен 0.0708547475232\n",
      "  А так покупкой доволен 0.0624569730414\n",
      "  я доволен 0.0152230509445\n",
      "3\n",
      "  Всем рекомендую 0.0436861951242\n",
      "  рекомендую 0.0276701894556\n",
      "  Рекомендую 0.0276701894556\n",
      "  НЕ РЕКОМЕНДУЮ 0.0276701894556\n",
      "4\n",
      "  Многие пишут о подобных пылесосах 0.119851280413\n",
      "  либо эйфория после покупки 0.029457750714\n",
      "  все на своих местах 0.0289422775335\n",
      "  Впечатления только положительные 0.0181231886304\n",
      "  легко разбирается-собирается 0.00944616457512\n",
      "  Результат на лицо 0.00562566809043\n",
      "  Конечно у него есть недостатки -0.012565752769\n",
      "  все понятно интуитивно -0.0153577136841\n",
      "  прост в обращении -0.0384648826049\n",
      "  правдой более 10 лет -0.117590180841\n",
      "5\n",
      "  Тяжеловат 0.0114954175562\n",
      "  Что тяжеловат 0.0114954175562\n",
      "  тяжеловат 0.0114954175562\n",
      "  Немного тяжеловат -0.0160429626245\n",
      "6\n",
      "  5 раза меньше чем стандартная) 0.0788140073663\n",
      "  тоже время тратится 0.0492634610485\n",
      "  все работает 0.0422777838631\n",
      "  большая проблема! Мощности хватает с лихвой 0.0400119186401\n",
      "  Мощности хватает 0.0310278331803\n",
      "  что для Вас это проблема 0.0147635677321\n",
      "  чтобы после сборки 0.0119389400137\n",
      "  сборки 0.0119389400137\n",
      "  эргономика 0.00516680426029\n",
      "  плюс 0.00356841048677\n",
      "  Привезла доставка 0.00166669958927\n",
      "  проще новый купить -0.0118371872987\n",
      "  Мощность вообще маленькая -0.016727044602\n",
      "  дизайн -0.0249409519017\n",
      "  качественная сборка внешний вид суперский -0.0400426382106\n",
      "  Получается -0.0430844773219\n",
      "  мощность -0.0453403950995\n",
      "  Сейчас практически не тянет -0.0503743807405\n",
      "  Качество пластика -0.0539659644034\n",
      "  более 10 кг -0.0587615866333\n",
      "  регулировка мощности -0.0632514926088\n",
      "7\n",
      "  а так очень хороший пылесос всем советую 0.0430150375474\n",
      "  очень прост в обслуживании 0.00535022479459\n",
      "  Берите не пожалеете! 0.00268398181272\n",
      "  Со своей работой справляется отлично -0.00361070299168\n",
      "  Покупка порадовала очень -0.0100437810937\n",
      "  До этого - все было прекрасно -0.0100775842753\n",
      "  В общем пока очень доволен -0.0113217749666\n",
      "  им очень довольны -0.0149807379582\n",
      "  что очень хорошо -0.0229250042591\n",
      "  не такая уж -0.0239169811447\n",
      "  на мой взгляд хорошая -0.026384537408\n",
      "  я думаю -0.0352942646591\n",
      "  я не думаю -0.0352942646591\n",
      "  что радует -0.0390317933185\n",
      "  Единственный недостаток немного тяжеловат -0.0420769000416\n",
      "  очень легко моется -0.0420818627703\n"
     ]
    }
   ],
   "source": [
    "for cluster, sentences in enumerate(sorted(test_product_aggregated_result.values(),\n",
    "                                 key=lambda x: sum(z[1] for z in x), reverse=True)):\n",
    "    print(cluster)\n",
    "    for sent, score in sorted(sentences, key=lambda x: x[1], reverse=True):\n",
    "        print(' ',sent, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
